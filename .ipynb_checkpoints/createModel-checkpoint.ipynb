{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_csv('input/bugzilla.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x):\n",
    "  x = x.astype(float)\n",
    "  min = np.min(x)\n",
    "  max = np.max(x)\n",
    "  return (x - min)/(max-min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_values(X, y, example):\n",
    "    label = y.loc[example]\n",
    "    image = X.loc[example,:].values.reshape([-1,1])\n",
    "    print(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of dataframe:  (4620, 17)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of dataframe: \", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transactionid</th>\n",
       "      <th>ns</th>\n",
       "      <th>nm</th>\n",
       "      <th>nf</th>\n",
       "      <th>entropy</th>\n",
       "      <th>la</th>\n",
       "      <th>ld</th>\n",
       "      <th>lt</th>\n",
       "      <th>fix</th>\n",
       "      <th>ndev</th>\n",
       "      <th>pd</th>\n",
       "      <th>npt</th>\n",
       "      <th>exp</th>\n",
       "      <th>rexp</th>\n",
       "      <th>sexp</th>\n",
       "      <th>bug</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4620.000000</td>\n",
       "      <td>4620.000000</td>\n",
       "      <td>4620.000000</td>\n",
       "      <td>4620.000000</td>\n",
       "      <td>4620.000000</td>\n",
       "      <td>4620.000000</td>\n",
       "      <td>4620.000000</td>\n",
       "      <td>4620.000000</td>\n",
       "      <td>4620.000000</td>\n",
       "      <td>4620.000000</td>\n",
       "      <td>4620.000000</td>\n",
       "      <td>4620.000000</td>\n",
       "      <td>4620.000000</td>\n",
       "      <td>4620.000000</td>\n",
       "      <td>4620.000000</td>\n",
       "      <td>4620.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5819.241558</td>\n",
       "      <td>1.169913</td>\n",
       "      <td>1.220346</td>\n",
       "      <td>2.287662</td>\n",
       "      <td>0.229153</td>\n",
       "      <td>0.071410</td>\n",
       "      <td>3.113758</td>\n",
       "      <td>591.379777</td>\n",
       "      <td>0.859957</td>\n",
       "      <td>16.424892</td>\n",
       "      <td>173.927922</td>\n",
       "      <td>0.952656</td>\n",
       "      <td>342.570563</td>\n",
       "      <td>253.033361</td>\n",
       "      <td>230.821429</td>\n",
       "      <td>0.367100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4223.181222</td>\n",
       "      <td>0.424315</td>\n",
       "      <td>0.569805</td>\n",
       "      <td>4.275243</td>\n",
       "      <td>0.371861</td>\n",
       "      <td>0.491756</td>\n",
       "      <td>198.624600</td>\n",
       "      <td>547.895977</td>\n",
       "      <td>0.347070</td>\n",
       "      <td>10.743430</td>\n",
       "      <td>646.256119</td>\n",
       "      <td>0.137676</td>\n",
       "      <td>392.273355</td>\n",
       "      <td>268.292945</td>\n",
       "      <td>269.729476</td>\n",
       "      <td>0.482066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2211.750000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003093</td>\n",
       "      <td>0.001295</td>\n",
       "      <td>210.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5193.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009909</td>\n",
       "      <td>0.005109</td>\n",
       "      <td>455.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>196.000000</td>\n",
       "      <td>156.000000</td>\n",
       "      <td>129.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>8804.250000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.551098</td>\n",
       "      <td>0.035356</td>\n",
       "      <td>0.017502</td>\n",
       "      <td>799.250000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>119.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>489.000000</td>\n",
       "      <td>352.000000</td>\n",
       "      <td>341.250000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>20938.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>13495.000000</td>\n",
       "      <td>2751.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>15836.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1815.000000</td>\n",
       "      <td>1042.750000</td>\n",
       "      <td>1741.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       transactionid           ns           nm           nf      entropy  \\\n",
       "count    4620.000000  4620.000000  4620.000000  4620.000000  4620.000000   \n",
       "mean     5819.241558     1.169913     1.220346     2.287662     0.229153   \n",
       "std      4223.181222     0.424315     0.569805     4.275243     0.371861   \n",
       "min         3.000000     1.000000     1.000000     1.000000     0.000000   \n",
       "25%      2211.750000     1.000000     1.000000     1.000000     0.000000   \n",
       "50%      5193.500000     1.000000     1.000000     1.000000     0.000000   \n",
       "75%      8804.250000     1.000000     1.000000     2.000000     0.551098   \n",
       "max     20938.000000     4.000000     8.000000    63.000000     1.000000   \n",
       "\n",
       "                la            ld           lt          fix         ndev  \\\n",
       "count  4620.000000   4620.000000  4620.000000  4620.000000  4620.000000   \n",
       "mean      0.071410      3.113758   591.379777     0.859957    16.424892   \n",
       "std       0.491756    198.624600   547.895977     0.347070    10.743430   \n",
       "min       0.000000      0.000000     0.000000     0.000000     1.000000   \n",
       "25%       0.003093      0.001295   210.000000     1.000000     7.000000   \n",
       "50%       0.009909      0.005109   455.000000     1.000000    16.000000   \n",
       "75%       0.035356      0.017502   799.250000     1.000000    24.000000   \n",
       "max      21.000000  13495.000000  2751.000000     1.000000    47.000000   \n",
       "\n",
       "                 pd          npt          exp         rexp         sexp  \\\n",
       "count   4620.000000  4620.000000  4620.000000  4620.000000  4620.000000   \n",
       "mean     173.927922     0.952656   342.570563   253.033361   230.821429   \n",
       "std      646.256119     0.137676   392.273355   268.292945   269.729476   \n",
       "min        0.000000     0.037037     1.000000     1.000000     1.000000   \n",
       "25%        3.000000     1.000000    58.000000    52.000000    35.000000   \n",
       "50%       20.000000     1.000000   196.000000   156.000000   129.500000   \n",
       "75%      119.000000     1.000000   489.000000   352.000000   341.250000   \n",
       "max    15836.000000     1.000000  1815.000000  1042.750000  1741.000000   \n",
       "\n",
       "               bug  \n",
       "count  4620.000000  \n",
       "mean      0.367100  \n",
       "std       0.482066  \n",
       "min       0.000000  \n",
       "25%       0.000000  \n",
       "50%       0.000000  \n",
       "75%       1.000000  \n",
       "max       1.000000  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RBM(object):\n",
    "    \n",
    "    def __init__(self, input_size, output_size, \n",
    "                 learning_rate, epochs, batchsize):\n",
    "\n",
    "        self._input_size = input_size\n",
    "        self._output_size = output_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.batchsize = batchsize\n",
    "        \n",
    "        # Initialize weights and biases using zero matrices\n",
    "        self.w = np.zeros([input_size, output_size], dtype=np.float32)\n",
    "        self.hb = np.zeros([output_size], dtype=np.float32)\n",
    "        self.vb = np.zeros([input_size], dtype=np.float32)\n",
    "    # forward pass, where h is the hidden layer and v is the visible layer\n",
    "    def prob_h_given_v(self, visible, w, hb):\n",
    "        return tf.nn.sigmoid(tf.matmul(visible, w) + hb)\n",
    "    # backward pass\n",
    "    def prob_v_given_h(self, hidden, w, vb):\n",
    "        return tf.nn.sigmoid(tf.matmul(hidden, tf.transpose(w)) + vb)\n",
    "    # sampling function\n",
    "    def sample_prob(self, probs):\n",
    "        return tf.nn.relu(tf.sign(probs - tf.random_uniform(tf.shape(probs))))\n",
    "\n",
    "    def train(self, X):\n",
    "        _w = tf.placeholder(tf.float32, [self._input_size, self._output_size])\n",
    "        _hb = tf.placeholder(tf.float32, [self._output_size])\n",
    "        _vb = tf.placeholder(tf.float32, [self._input_size])\n",
    "        \n",
    "        prv_w = np.zeros([self._input_size, self._output_size], dtype=np.float32)\n",
    "        prv_hb = np.zeros([self._output_size], dtype=np.float32)\n",
    "        prv_vb = np.zeros([self._input_size], dtype=np.float32)\n",
    "        \n",
    "        cur_w = np.zeros([self._input_size, self._output_size], dtype=np.float32)\n",
    "        cur_hb = np.zeros([self._output_size], dtype=np.float32)\n",
    "        cur_vb = np.zeros([self._input_size], dtype=np.float32)\n",
    "        \n",
    "        v0 = tf.placeholder(tf.float32, [None, self._input_size])\n",
    "        h0 = self.sample_prob(self.prob_h_given_v(v0, _w, _hb))\n",
    "        v1 = self.sample_prob(self.prob_v_given_h(h0, _w, _vb))\n",
    "        h1 = self.prob_h_given_v(v1, _w, _hb)\n",
    "        #To update the weights, we perform constrastive divergence.\n",
    "        positive_grad = tf.matmul(tf.transpose(v0), h0)\n",
    "        negative_grad = tf.matmul(tf.transpose(v1), h1)\n",
    "        \n",
    "        update_w = _w + self.learning_rate * (positive_grad - negative_grad) / tf.to_float(tf.shape(v0)[0])\n",
    "        update_vb = _vb +  self.learning_rate * tf.reduce_mean(v0 - v1, 0)\n",
    "        update_hb = _hb +  self.learning_rate * tf.reduce_mean(h0 - h1, 0)\n",
    "        # We also define the error as the MSE\n",
    "        err = tf.reduce_mean(tf.square(v0 - v1))\n",
    "        \n",
    "        error_list = []\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            \n",
    "            for epoch in range(self.epochs):\n",
    "                for start, end in zip(range(0, len(X),  self.batchsize),range(self.batchsize,len(X), self.batchsize)):\n",
    "                    batch = X[start:end]\n",
    "                    cur_w = sess.run(update_w, feed_dict={v0: batch, _w: prv_w, _hb: prv_hb, _vb: prv_vb})\n",
    "                    cur_hb = sess.run(update_hb, feed_dict={v0: batch,  _w: prv_w, _hb: prv_hb, _vb: prv_vb})\n",
    "                    cur_vb = sess.run(update_vb, feed_dict={v0: batch, _w: prv_w, _hb: prv_hb, _vb: prv_vb})\n",
    "                    prv_w = cur_w\n",
    "                    prv_hb = cur_hb\n",
    "                    prv_vb = cur_vb\n",
    "                error = sess.run(err, feed_dict={v0: X, _w: cur_w, _vb: cur_vb, _hb: cur_hb})\n",
    "                print ('Epoch: %d' % epoch,'reconstruction error: %f' % error)\n",
    "                error_list.append(error)\n",
    "            self.w = prv_w\n",
    "            self.hb = prv_hb\n",
    "            self.vb = prv_vb\n",
    "            return error_list\n",
    "    #function to generate new features from the generative model that the RBM has learned\n",
    "    def rbm_output(self, X):\n",
    "        \n",
    "        input_X = tf.constant(X)\n",
    "        _w = tf.constant(self.w)\n",
    "        _hb = tf.constant(self.hb)\n",
    "        _vb = tf.constant(self.vb)\n",
    "        out = tf.nn.sigmoid(tf.matmul(input_X, _w) + _hb)\n",
    "        hiddenGen = self.sample_prob(self.prob_h_given_v(input_X, _w, _hb))\n",
    "        visibleGen = self.sample_prob(self.prob_v_given_h(hiddenGen, _w, _vb))\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            return sess.run(out), sess.run(visibleGen), sess.run(hiddenGen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.drop(['commitdate','transactionid'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = df.iloc[:,:-1].apply(func=normalize, axis=0)\n",
    "train_Y = df.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ns  nm  nf   entropy        la        ld          lt  fix  ndev     pd  \\\n",
      "0   1   1   3  0.579380  0.093620  0.000000  480.666667    1    14    596   \n",
      "1   1   1   1  0.000000  0.000000  0.000000  398.000000    1     1      0   \n",
      "2   3   3  52  0.739279  0.183477  0.208913  283.519231    0    23  15836   \n",
      "3   1   1   8  0.685328  0.016039  0.012880  514.375000    1    21   1281   \n",
      "4   2   2  38  0.769776  0.091829  0.072746  366.815789    1    21   6565   \n",
      "\n",
      "        npt  exp    rexp  sexp  bug  \n",
      "0  0.666667  143  133.50   129    1  \n",
      "1  1.000000  140  140.00   137    1  \n",
      "2  0.750000  984  818.65   978    0  \n",
      "3  1.000000  579  479.25   550    0  \n",
      "4  0.763158  413  313.25   405    0  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4620, 15)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.head())\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "inputX = df.iloc[:,:-1].apply(func=normalize, axis=0).values\n",
    "inputY= df.iloc[:,-1].values\n",
    "print(type(inputX))\n",
    "inputX = inputX.astype(np.float32)\n",
    "\n",
    "#List to hold RBMs\n",
    "rbm_list = []\n",
    "\n",
    "#define parameters of RBMs we will train\n",
    "# 14-20-12-12-2\n",
    "\n",
    "rbm_list.append(RBM(14, 20, 0.002, 200, 100))\n",
    "rbm_list.append(RBM(20, 12, 0.002, 200, 100))\n",
    "rbm_list.append(RBM(12, 12, 0.002, 200, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\akib1\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RBM 1\n",
      "WARNING:tensorflow:From <ipython-input-14-cd4b7305a71f>:47: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "Epoch: 0 reconstruction error: 0.405829\n",
      "Epoch: 1 reconstruction error: 0.371015\n",
      "Epoch: 2 reconstruction error: 0.342177\n",
      "Epoch: 3 reconstruction error: 0.317912\n",
      "Epoch: 4 reconstruction error: 0.294592\n",
      "Epoch: 5 reconstruction error: 0.277995\n",
      "Epoch: 6 reconstruction error: 0.262601\n",
      "Epoch: 7 reconstruction error: 0.249755\n",
      "Epoch: 8 reconstruction error: 0.236498\n",
      "Epoch: 9 reconstruction error: 0.229906\n",
      "Epoch: 10 reconstruction error: 0.219831\n",
      "Epoch: 11 reconstruction error: 0.210117\n",
      "Epoch: 12 reconstruction error: 0.205498\n",
      "Epoch: 13 reconstruction error: 0.199783\n",
      "Epoch: 14 reconstruction error: 0.193146\n",
      "Epoch: 15 reconstruction error: 0.186822\n",
      "Epoch: 16 reconstruction error: 0.182999\n",
      "Epoch: 17 reconstruction error: 0.181354\n",
      "Epoch: 18 reconstruction error: 0.175588\n",
      "Epoch: 19 reconstruction error: 0.173575\n",
      "Epoch: 20 reconstruction error: 0.171473\n",
      "Epoch: 21 reconstruction error: 0.168823\n",
      "Epoch: 22 reconstruction error: 0.166191\n",
      "Epoch: 23 reconstruction error: 0.164092\n",
      "Epoch: 24 reconstruction error: 0.162714\n",
      "Epoch: 25 reconstruction error: 0.160940\n",
      "Epoch: 26 reconstruction error: 0.156908\n",
      "Epoch: 27 reconstruction error: 0.157490\n",
      "Epoch: 28 reconstruction error: 0.155882\n",
      "Epoch: 29 reconstruction error: 0.154590\n",
      "Epoch: 30 reconstruction error: 0.155267\n",
      "Epoch: 31 reconstruction error: 0.152582\n",
      "Epoch: 32 reconstruction error: 0.150335\n",
      "Epoch: 33 reconstruction error: 0.151555\n",
      "Epoch: 34 reconstruction error: 0.148796\n",
      "Epoch: 35 reconstruction error: 0.149000\n",
      "Epoch: 36 reconstruction error: 0.148981\n",
      "Epoch: 37 reconstruction error: 0.147436\n",
      "Epoch: 38 reconstruction error: 0.145894\n",
      "Epoch: 39 reconstruction error: 0.145890\n",
      "Epoch: 40 reconstruction error: 0.144078\n",
      "Epoch: 41 reconstruction error: 0.143646\n",
      "Epoch: 42 reconstruction error: 0.142970\n",
      "Epoch: 43 reconstruction error: 0.144397\n",
      "Epoch: 44 reconstruction error: 0.143358\n",
      "Epoch: 45 reconstruction error: 0.143939\n",
      "Epoch: 46 reconstruction error: 0.144257\n",
      "Epoch: 47 reconstruction error: 0.143184\n",
      "Epoch: 48 reconstruction error: 0.140382\n",
      "Epoch: 49 reconstruction error: 0.141880\n",
      "Epoch: 50 reconstruction error: 0.140017\n",
      "Epoch: 51 reconstruction error: 0.141156\n",
      "Epoch: 52 reconstruction error: 0.139015\n",
      "Epoch: 53 reconstruction error: 0.141938\n",
      "Epoch: 54 reconstruction error: 0.138929\n",
      "Epoch: 55 reconstruction error: 0.139785\n",
      "Epoch: 56 reconstruction error: 0.141379\n",
      "Epoch: 57 reconstruction error: 0.138416\n",
      "Epoch: 58 reconstruction error: 0.139903\n",
      "Epoch: 59 reconstruction error: 0.139001\n",
      "Epoch: 60 reconstruction error: 0.140977\n",
      "Epoch: 61 reconstruction error: 0.138768\n",
      "Epoch: 62 reconstruction error: 0.138223\n",
      "Epoch: 63 reconstruction error: 0.137449\n",
      "Epoch: 64 reconstruction error: 0.139284\n",
      "Epoch: 65 reconstruction error: 0.137586\n",
      "Epoch: 66 reconstruction error: 0.137880\n",
      "Epoch: 67 reconstruction error: 0.137114\n",
      "Epoch: 68 reconstruction error: 0.138924\n",
      "Epoch: 69 reconstruction error: 0.137551\n",
      "Epoch: 70 reconstruction error: 0.138158\n",
      "Epoch: 71 reconstruction error: 0.137690\n",
      "Epoch: 72 reconstruction error: 0.136826\n",
      "Epoch: 73 reconstruction error: 0.136698\n",
      "Epoch: 74 reconstruction error: 0.133893\n",
      "Epoch: 75 reconstruction error: 0.136785\n",
      "Epoch: 76 reconstruction error: 0.137429\n",
      "Epoch: 77 reconstruction error: 0.136075\n",
      "Epoch: 78 reconstruction error: 0.135822\n",
      "Epoch: 79 reconstruction error: 0.134824\n",
      "Epoch: 80 reconstruction error: 0.137240\n",
      "Epoch: 81 reconstruction error: 0.135413\n",
      "Epoch: 82 reconstruction error: 0.137809\n",
      "Epoch: 83 reconstruction error: 0.135883\n",
      "Epoch: 84 reconstruction error: 0.135064\n",
      "Epoch: 85 reconstruction error: 0.135644\n",
      "Epoch: 86 reconstruction error: 0.136939\n",
      "Epoch: 87 reconstruction error: 0.136126\n",
      "Epoch: 88 reconstruction error: 0.135461\n",
      "Epoch: 89 reconstruction error: 0.137839\n",
      "Epoch: 90 reconstruction error: 0.134144\n",
      "Epoch: 91 reconstruction error: 0.135656\n",
      "Epoch: 92 reconstruction error: 0.134273\n",
      "Epoch: 93 reconstruction error: 0.134820\n",
      "Epoch: 94 reconstruction error: 0.132567\n",
      "Epoch: 95 reconstruction error: 0.135033\n",
      "Epoch: 96 reconstruction error: 0.135448\n",
      "Epoch: 97 reconstruction error: 0.133956\n",
      "Epoch: 98 reconstruction error: 0.134933\n",
      "Epoch: 99 reconstruction error: 0.133779\n",
      "Epoch: 100 reconstruction error: 0.134362\n",
      "Epoch: 101 reconstruction error: 0.134209\n",
      "Epoch: 102 reconstruction error: 0.133336\n",
      "Epoch: 103 reconstruction error: 0.133586\n",
      "Epoch: 104 reconstruction error: 0.133898\n",
      "Epoch: 105 reconstruction error: 0.134021\n",
      "Epoch: 106 reconstruction error: 0.134070\n",
      "Epoch: 107 reconstruction error: 0.135692\n",
      "Epoch: 108 reconstruction error: 0.135267\n",
      "Epoch: 109 reconstruction error: 0.134597\n",
      "Epoch: 110 reconstruction error: 0.132461\n",
      "Epoch: 111 reconstruction error: 0.134396\n",
      "Epoch: 112 reconstruction error: 0.133803\n",
      "Epoch: 113 reconstruction error: 0.136024\n",
      "Epoch: 114 reconstruction error: 0.132982\n",
      "Epoch: 115 reconstruction error: 0.133935\n",
      "Epoch: 116 reconstruction error: 0.132930\n",
      "Epoch: 117 reconstruction error: 0.133049\n",
      "Epoch: 118 reconstruction error: 0.134154\n",
      "Epoch: 119 reconstruction error: 0.134313\n",
      "Epoch: 120 reconstruction error: 0.134121\n",
      "Epoch: 121 reconstruction error: 0.134987\n",
      "Epoch: 122 reconstruction error: 0.134148\n",
      "Epoch: 123 reconstruction error: 0.133412\n",
      "Epoch: 124 reconstruction error: 0.135157\n",
      "Epoch: 125 reconstruction error: 0.133131\n",
      "Epoch: 126 reconstruction error: 0.133972\n",
      "Epoch: 127 reconstruction error: 0.131859\n",
      "Epoch: 128 reconstruction error: 0.131952\n",
      "Epoch: 129 reconstruction error: 0.133977\n",
      "Epoch: 130 reconstruction error: 0.133474\n",
      "Epoch: 131 reconstruction error: 0.134947\n",
      "Epoch: 132 reconstruction error: 0.132107\n",
      "Epoch: 133 reconstruction error: 0.133580\n",
      "Epoch: 134 reconstruction error: 0.133594\n",
      "Epoch: 135 reconstruction error: 0.134167\n",
      "Epoch: 136 reconstruction error: 0.132005\n",
      "Epoch: 137 reconstruction error: 0.133829\n",
      "Epoch: 138 reconstruction error: 0.132934\n",
      "Epoch: 139 reconstruction error: 0.133082\n",
      "Epoch: 140 reconstruction error: 0.132761\n",
      "Epoch: 141 reconstruction error: 0.133198\n",
      "Epoch: 142 reconstruction error: 0.133740\n",
      "Epoch: 143 reconstruction error: 0.132279\n",
      "Epoch: 144 reconstruction error: 0.131504\n",
      "Epoch: 145 reconstruction error: 0.133401\n",
      "Epoch: 146 reconstruction error: 0.134459\n",
      "Epoch: 147 reconstruction error: 0.132749\n",
      "Epoch: 148 reconstruction error: 0.133283\n",
      "Epoch: 149 reconstruction error: 0.133370\n",
      "Epoch: 150 reconstruction error: 0.130956\n",
      "Epoch: 151 reconstruction error: 0.133862\n",
      "Epoch: 152 reconstruction error: 0.133974\n",
      "Epoch: 153 reconstruction error: 0.132147\n",
      "Epoch: 154 reconstruction error: 0.132730\n",
      "Epoch: 155 reconstruction error: 0.134172\n",
      "Epoch: 156 reconstruction error: 0.134132\n",
      "Epoch: 157 reconstruction error: 0.134426\n",
      "Epoch: 158 reconstruction error: 0.133291\n",
      "Epoch: 159 reconstruction error: 0.132817\n",
      "Epoch: 160 reconstruction error: 0.133098\n",
      "Epoch: 161 reconstruction error: 0.132913\n",
      "Epoch: 162 reconstruction error: 0.131660\n",
      "Epoch: 163 reconstruction error: 0.133750\n",
      "Epoch: 164 reconstruction error: 0.131964\n",
      "Epoch: 165 reconstruction error: 0.131388\n",
      "Epoch: 166 reconstruction error: 0.132356\n",
      "Epoch: 167 reconstruction error: 0.132301\n",
      "Epoch: 168 reconstruction error: 0.132437\n",
      "Epoch: 169 reconstruction error: 0.132213\n",
      "Epoch: 170 reconstruction error: 0.131219\n",
      "Epoch: 171 reconstruction error: 0.132842\n",
      "Epoch: 172 reconstruction error: 0.131820\n",
      "Epoch: 173 reconstruction error: 0.130823\n",
      "Epoch: 174 reconstruction error: 0.134418\n",
      "Epoch: 175 reconstruction error: 0.131696\n",
      "Epoch: 176 reconstruction error: 0.132442\n",
      "Epoch: 177 reconstruction error: 0.132760\n",
      "Epoch: 178 reconstruction error: 0.131939\n",
      "Epoch: 179 reconstruction error: 0.131520\n",
      "Epoch: 180 reconstruction error: 0.131916\n",
      "Epoch: 181 reconstruction error: 0.131456\n",
      "Epoch: 182 reconstruction error: 0.132505\n",
      "Epoch: 183 reconstruction error: 0.130907\n",
      "Epoch: 184 reconstruction error: 0.131306\n",
      "Epoch: 185 reconstruction error: 0.131157\n",
      "Epoch: 186 reconstruction error: 0.131147\n",
      "Epoch: 187 reconstruction error: 0.130522\n",
      "Epoch: 188 reconstruction error: 0.132306\n",
      "Epoch: 189 reconstruction error: 0.131315\n",
      "Epoch: 190 reconstruction error: 0.132340\n",
      "Epoch: 191 reconstruction error: 0.131631\n",
      "Epoch: 192 reconstruction error: 0.130132\n",
      "Epoch: 193 reconstruction error: 0.131386\n",
      "Epoch: 194 reconstruction error: 0.133429\n",
      "Epoch: 195 reconstruction error: 0.132698\n",
      "Epoch: 196 reconstruction error: 0.132666\n",
      "Epoch: 197 reconstruction error: 0.134019\n",
      "Epoch: 198 reconstruction error: 0.131015\n",
      "Epoch: 199 reconstruction error: 0.131649\n",
      "RBM 2\n",
      "Epoch: 0 reconstruction error: 0.498517\n",
      "Epoch: 1 reconstruction error: 0.497035\n",
      "Epoch: 2 reconstruction error: 0.494686\n",
      "Epoch: 3 reconstruction error: 0.495206\n",
      "Epoch: 4 reconstruction error: 0.490498\n",
      "Epoch: 5 reconstruction error: 0.490671\n",
      "Epoch: 6 reconstruction error: 0.490335\n",
      "Epoch: 7 reconstruction error: 0.487716\n",
      "Epoch: 8 reconstruction error: 0.484448\n",
      "Epoch: 9 reconstruction error: 0.486006\n",
      "Epoch: 10 reconstruction error: 0.486623\n",
      "Epoch: 11 reconstruction error: 0.482067\n",
      "Epoch: 12 reconstruction error: 0.481688\n",
      "Epoch: 13 reconstruction error: 0.484372\n",
      "Epoch: 14 reconstruction error: 0.483236\n",
      "Epoch: 15 reconstruction error: 0.483420\n",
      "Epoch: 16 reconstruction error: 0.481894\n",
      "Epoch: 17 reconstruction error: 0.480563\n",
      "Epoch: 18 reconstruction error: 0.484610\n",
      "Epoch: 19 reconstruction error: 0.480801\n",
      "Epoch: 20 reconstruction error: 0.480552\n",
      "Epoch: 21 reconstruction error: 0.477955\n",
      "Epoch: 22 reconstruction error: 0.485032\n",
      "Epoch: 23 reconstruction error: 0.480779\n",
      "Epoch: 24 reconstruction error: 0.479903\n",
      "Epoch: 25 reconstruction error: 0.482933\n",
      "Epoch: 26 reconstruction error: 0.477316\n",
      "Epoch: 27 reconstruction error: 0.480996\n",
      "Epoch: 28 reconstruction error: 0.479848\n",
      "Epoch: 29 reconstruction error: 0.478918\n",
      "Epoch: 30 reconstruction error: 0.479545\n",
      "Epoch: 31 reconstruction error: 0.478312\n",
      "Epoch: 32 reconstruction error: 0.474556\n",
      "Epoch: 33 reconstruction error: 0.480725\n",
      "Epoch: 34 reconstruction error: 0.479968\n",
      "Epoch: 35 reconstruction error: 0.476450\n",
      "Epoch: 36 reconstruction error: 0.480292\n",
      "Epoch: 37 reconstruction error: 0.479167\n",
      "Epoch: 38 reconstruction error: 0.481558\n",
      "Epoch: 39 reconstruction error: 0.480790\n",
      "Epoch: 40 reconstruction error: 0.480357\n",
      "Epoch: 41 reconstruction error: 0.478950\n",
      "Epoch: 42 reconstruction error: 0.480801\n",
      "Epoch: 43 reconstruction error: 0.480216\n",
      "Epoch: 44 reconstruction error: 0.480660\n",
      "Epoch: 45 reconstruction error: 0.480336\n",
      "Epoch: 46 reconstruction error: 0.477890\n",
      "Epoch: 47 reconstruction error: 0.480628\n",
      "Epoch: 48 reconstruction error: 0.480357\n",
      "Epoch: 49 reconstruction error: 0.482738\n",
      "Epoch: 50 reconstruction error: 0.478452\n",
      "Epoch: 51 reconstruction error: 0.477305\n",
      "Epoch: 52 reconstruction error: 0.481104\n",
      "Epoch: 53 reconstruction error: 0.480292\n",
      "Epoch: 54 reconstruction error: 0.481840\n",
      "Epoch: 55 reconstruction error: 0.479968\n",
      "Epoch: 56 reconstruction error: 0.482143\n",
      "Epoch: 57 reconstruction error: 0.478236\n",
      "Epoch: 58 reconstruction error: 0.480130\n",
      "Epoch: 59 reconstruction error: 0.482857\n",
      "Epoch: 60 reconstruction error: 0.483312\n",
      "Epoch: 61 reconstruction error: 0.480141\n",
      "Epoch: 62 reconstruction error: 0.480346\n",
      "Epoch: 63 reconstruction error: 0.476342\n",
      "Epoch: 64 reconstruction error: 0.481190\n",
      "Epoch: 65 reconstruction error: 0.478485\n",
      "Epoch: 66 reconstruction error: 0.480952\n",
      "Epoch: 67 reconstruction error: 0.480498\n",
      "Epoch: 68 reconstruction error: 0.481742\n",
      "Epoch: 69 reconstruction error: 0.480509\n",
      "Epoch: 70 reconstruction error: 0.478301\n",
      "Epoch: 71 reconstruction error: 0.481494\n",
      "Epoch: 72 reconstruction error: 0.478084\n",
      "Epoch: 73 reconstruction error: 0.477478\n",
      "Epoch: 74 reconstruction error: 0.482284\n",
      "Epoch: 75 reconstruction error: 0.479491\n",
      "Epoch: 76 reconstruction error: 0.479448\n",
      "Epoch: 77 reconstruction error: 0.479264\n",
      "Epoch: 78 reconstruction error: 0.478095\n",
      "Epoch: 79 reconstruction error: 0.478528\n",
      "Epoch: 80 reconstruction error: 0.481515\n",
      "Epoch: 81 reconstruction error: 0.479156\n",
      "Epoch: 82 reconstruction error: 0.480238\n",
      "Epoch: 83 reconstruction error: 0.479827\n",
      "Epoch: 84 reconstruction error: 0.478366\n",
      "Epoch: 85 reconstruction error: 0.481699\n",
      "Epoch: 86 reconstruction error: 0.483312\n",
      "Epoch: 87 reconstruction error: 0.480682\n",
      "Epoch: 88 reconstruction error: 0.478301\n",
      "Epoch: 89 reconstruction error: 0.477814\n",
      "Epoch: 90 reconstruction error: 0.482100\n",
      "Epoch: 91 reconstruction error: 0.481385\n",
      "Epoch: 92 reconstruction error: 0.480195\n",
      "Epoch: 93 reconstruction error: 0.478063\n",
      "Epoch: 94 reconstruction error: 0.481926\n",
      "Epoch: 95 reconstruction error: 0.481558\n",
      "Epoch: 96 reconstruction error: 0.477641\n",
      "Epoch: 97 reconstruction error: 0.480281\n",
      "Epoch: 98 reconstruction error: 0.479275\n",
      "Epoch: 99 reconstruction error: 0.481071\n",
      "Epoch: 100 reconstruction error: 0.478929\n",
      "Epoch: 101 reconstruction error: 0.480011\n",
      "Epoch: 102 reconstruction error: 0.478939\n",
      "Epoch: 103 reconstruction error: 0.482890\n",
      "Epoch: 104 reconstruction error: 0.479307\n",
      "Epoch: 105 reconstruction error: 0.483517\n",
      "Epoch: 106 reconstruction error: 0.479989\n",
      "Epoch: 107 reconstruction error: 0.480487\n",
      "Epoch: 108 reconstruction error: 0.480314\n",
      "Epoch: 109 reconstruction error: 0.481147\n",
      "Epoch: 110 reconstruction error: 0.481093\n",
      "Epoch: 111 reconstruction error: 0.481158\n",
      "Epoch: 112 reconstruction error: 0.476201\n",
      "Epoch: 113 reconstruction error: 0.479740\n",
      "Epoch: 114 reconstruction error: 0.479708\n",
      "Epoch: 115 reconstruction error: 0.479286\n",
      "Epoch: 116 reconstruction error: 0.480920\n",
      "Epoch: 117 reconstruction error: 0.479892\n",
      "Epoch: 118 reconstruction error: 0.481732\n",
      "Epoch: 119 reconstruction error: 0.479675\n",
      "Epoch: 120 reconstruction error: 0.478561\n",
      "Epoch: 121 reconstruction error: 0.483063\n",
      "Epoch: 122 reconstruction error: 0.478474\n",
      "Epoch: 123 reconstruction error: 0.479881\n",
      "Epoch: 124 reconstruction error: 0.481948\n",
      "Epoch: 125 reconstruction error: 0.478755\n",
      "Epoch: 126 reconstruction error: 0.480963\n",
      "Epoch: 127 reconstruction error: 0.476645\n",
      "Epoch: 128 reconstruction error: 0.479481\n",
      "Epoch: 129 reconstruction error: 0.478171\n",
      "Epoch: 130 reconstruction error: 0.478820\n",
      "Epoch: 131 reconstruction error: 0.478918\n",
      "Epoch: 132 reconstruction error: 0.477597\n",
      "Epoch: 133 reconstruction error: 0.477911\n",
      "Epoch: 134 reconstruction error: 0.479481\n",
      "Epoch: 135 reconstruction error: 0.479545\n",
      "Epoch: 136 reconstruction error: 0.481299\n",
      "Epoch: 137 reconstruction error: 0.480628\n",
      "Epoch: 138 reconstruction error: 0.479275\n",
      "Epoch: 139 reconstruction error: 0.478896\n",
      "Epoch: 140 reconstruction error: 0.479816\n",
      "Epoch: 141 reconstruction error: 0.477587\n",
      "Epoch: 142 reconstruction error: 0.480119\n",
      "Epoch: 143 reconstruction error: 0.478507\n",
      "Epoch: 144 reconstruction error: 0.481201\n",
      "Epoch: 145 reconstruction error: 0.478896\n",
      "Epoch: 146 reconstruction error: 0.480833\n",
      "Epoch: 147 reconstruction error: 0.481461\n",
      "Epoch: 148 reconstruction error: 0.481234\n",
      "Epoch: 149 reconstruction error: 0.481234\n",
      "Epoch: 150 reconstruction error: 0.481331\n",
      "Epoch: 151 reconstruction error: 0.478745\n",
      "Epoch: 152 reconstruction error: 0.479188\n",
      "Epoch: 153 reconstruction error: 0.479567\n",
      "Epoch: 154 reconstruction error: 0.481732\n",
      "Epoch: 155 reconstruction error: 0.480920\n",
      "Epoch: 156 reconstruction error: 0.478019\n",
      "Epoch: 157 reconstruction error: 0.478810\n",
      "Epoch: 158 reconstruction error: 0.476613\n",
      "Epoch: 159 reconstruction error: 0.482662\n",
      "Epoch: 160 reconstruction error: 0.478712\n",
      "Epoch: 161 reconstruction error: 0.483149\n",
      "Epoch: 162 reconstruction error: 0.480357\n",
      "Epoch: 163 reconstruction error: 0.479643\n",
      "Epoch: 164 reconstruction error: 0.479134\n",
      "Epoch: 165 reconstruction error: 0.480931\n",
      "Epoch: 166 reconstruction error: 0.480271\n",
      "Epoch: 167 reconstruction error: 0.481310\n",
      "Epoch: 168 reconstruction error: 0.479697\n",
      "Epoch: 169 reconstruction error: 0.479784\n",
      "Epoch: 170 reconstruction error: 0.477619\n",
      "Epoch: 171 reconstruction error: 0.479686\n",
      "Epoch: 172 reconstruction error: 0.477262\n",
      "Epoch: 173 reconstruction error: 0.479924\n",
      "Epoch: 174 reconstruction error: 0.479091\n",
      "Epoch: 175 reconstruction error: 0.480097\n",
      "Epoch: 176 reconstruction error: 0.479957\n",
      "Epoch: 177 reconstruction error: 0.479275\n",
      "Epoch: 178 reconstruction error: 0.477511\n",
      "Epoch: 179 reconstruction error: 0.478593\n",
      "Epoch: 180 reconstruction error: 0.480887\n",
      "Epoch: 181 reconstruction error: 0.480130\n",
      "Epoch: 182 reconstruction error: 0.483323\n",
      "Epoch: 183 reconstruction error: 0.479989\n",
      "Epoch: 184 reconstruction error: 0.481991\n",
      "Epoch: 185 reconstruction error: 0.481169\n",
      "Epoch: 186 reconstruction error: 0.480368\n",
      "Epoch: 187 reconstruction error: 0.477208\n",
      "Epoch: 188 reconstruction error: 0.479048\n",
      "Epoch: 189 reconstruction error: 0.479838\n",
      "Epoch: 190 reconstruction error: 0.482229\n",
      "Epoch: 191 reconstruction error: 0.480076\n",
      "Epoch: 192 reconstruction error: 0.482121\n",
      "Epoch: 193 reconstruction error: 0.481797\n",
      "Epoch: 194 reconstruction error: 0.479275\n",
      "Epoch: 195 reconstruction error: 0.480779\n",
      "Epoch: 196 reconstruction error: 0.480119\n",
      "Epoch: 197 reconstruction error: 0.480487\n",
      "Epoch: 198 reconstruction error: 0.478593\n",
      "Epoch: 199 reconstruction error: 0.480628\n",
      "RBM 3\n",
      "Epoch: 0 reconstruction error: 0.499188\n",
      "Epoch: 1 reconstruction error: 0.495310\n",
      "Epoch: 2 reconstruction error: 0.490206\n",
      "Epoch: 3 reconstruction error: 0.489051\n",
      "Epoch: 4 reconstruction error: 0.481097\n",
      "Epoch: 5 reconstruction error: 0.482540\n",
      "Epoch: 6 reconstruction error: 0.479311\n",
      "Epoch: 7 reconstruction error: 0.478878\n",
      "Epoch: 8 reconstruction error: 0.476209\n",
      "Epoch: 9 reconstruction error: 0.473918\n",
      "Epoch: 10 reconstruction error: 0.473810\n",
      "Epoch: 11 reconstruction error: 0.474152\n",
      "Epoch: 12 reconstruction error: 0.469697\n",
      "Epoch: 13 reconstruction error: 0.467190\n",
      "Epoch: 14 reconstruction error: 0.472042\n",
      "Epoch: 15 reconstruction error: 0.471573\n",
      "Epoch: 16 reconstruction error: 0.468074\n",
      "Epoch: 17 reconstruction error: 0.470905\n",
      "Epoch: 18 reconstruction error: 0.468723\n",
      "Epoch: 19 reconstruction error: 0.467532\n",
      "Epoch: 20 reconstruction error: 0.468525\n",
      "Epoch: 21 reconstruction error: 0.468362\n",
      "Epoch: 22 reconstruction error: 0.470058\n",
      "Epoch: 23 reconstruction error: 0.465494\n",
      "Epoch: 24 reconstruction error: 0.462446\n",
      "Epoch: 25 reconstruction error: 0.466216\n",
      "Epoch: 26 reconstruction error: 0.465837\n",
      "Epoch: 27 reconstruction error: 0.464538\n",
      "Epoch: 28 reconstruction error: 0.468019\n",
      "Epoch: 29 reconstruction error: 0.465025\n",
      "Epoch: 30 reconstruction error: 0.464773\n",
      "Epoch: 31 reconstruction error: 0.467839\n",
      "Epoch: 32 reconstruction error: 0.463745\n",
      "Epoch: 33 reconstruction error: 0.462103\n",
      "Epoch: 34 reconstruction error: 0.466053\n",
      "Epoch: 35 reconstruction error: 0.462157\n",
      "Epoch: 36 reconstruction error: 0.460588\n",
      "Epoch: 37 reconstruction error: 0.464719\n",
      "Epoch: 38 reconstruction error: 0.465747\n",
      "Epoch: 39 reconstruction error: 0.463456\n",
      "Epoch: 40 reconstruction error: 0.467514\n",
      "Epoch: 41 reconstruction error: 0.464538\n",
      "Epoch: 42 reconstruction error: 0.463131\n",
      "Epoch: 43 reconstruction error: 0.464466\n",
      "Epoch: 44 reconstruction error: 0.467298\n",
      "Epoch: 45 reconstruction error: 0.465422\n",
      "Epoch: 46 reconstruction error: 0.464989\n",
      "Epoch: 47 reconstruction error: 0.464989\n",
      "Epoch: 48 reconstruction error: 0.466378\n",
      "Epoch: 49 reconstruction error: 0.463763\n",
      "Epoch: 50 reconstruction error: 0.462897\n",
      "Epoch: 51 reconstruction error: 0.462825\n",
      "Epoch: 52 reconstruction error: 0.466414\n",
      "Epoch: 53 reconstruction error: 0.464484\n",
      "Epoch: 54 reconstruction error: 0.464628\n",
      "Epoch: 55 reconstruction error: 0.460913\n",
      "Epoch: 56 reconstruction error: 0.464448\n",
      "Epoch: 57 reconstruction error: 0.466108\n",
      "Epoch: 58 reconstruction error: 0.464971\n",
      "Epoch: 59 reconstruction error: 0.466053\n",
      "Epoch: 60 reconstruction error: 0.462807\n",
      "Epoch: 61 reconstruction error: 0.464628\n",
      "Epoch: 62 reconstruction error: 0.461634\n",
      "Epoch: 63 reconstruction error: 0.464989\n",
      "Epoch: 64 reconstruction error: 0.459253\n",
      "Epoch: 65 reconstruction error: 0.461382\n",
      "Epoch: 66 reconstruction error: 0.466631\n",
      "Epoch: 67 reconstruction error: 0.467100\n",
      "Epoch: 68 reconstruction error: 0.465386\n",
      "Epoch: 69 reconstruction error: 0.461652\n",
      "Epoch: 70 reconstruction error: 0.464953\n",
      "Epoch: 71 reconstruction error: 0.464610\n",
      "Epoch: 72 reconstruction error: 0.464033\n",
      "Epoch: 73 reconstruction error: 0.462987\n",
      "Epoch: 74 reconstruction error: 0.465350\n",
      "Epoch: 75 reconstruction error: 0.464448\n",
      "Epoch: 76 reconstruction error: 0.466144\n",
      "Epoch: 77 reconstruction error: 0.462969\n",
      "Epoch: 78 reconstruction error: 0.466162\n",
      "Epoch: 79 reconstruction error: 0.466378\n",
      "Epoch: 80 reconstruction error: 0.465188\n",
      "Epoch: 81 reconstruction error: 0.464286\n",
      "Epoch: 82 reconstruction error: 0.465765\n",
      "Epoch: 83 reconstruction error: 0.461400\n",
      "Epoch: 84 reconstruction error: 0.465963\n",
      "Epoch: 85 reconstruction error: 0.466360\n",
      "Epoch: 86 reconstruction error: 0.464899\n",
      "Epoch: 87 reconstruction error: 0.463979\n",
      "Epoch: 88 reconstruction error: 0.465242\n",
      "Epoch: 89 reconstruction error: 0.462753\n",
      "Epoch: 90 reconstruction error: 0.465530\n",
      "Epoch: 91 reconstruction error: 0.466180\n",
      "Epoch: 92 reconstruction error: 0.465115\n",
      "Epoch: 93 reconstruction error: 0.465963\n",
      "Epoch: 94 reconstruction error: 0.463167\n",
      "Epoch: 95 reconstruction error: 0.465007\n",
      "Epoch: 96 reconstruction error: 0.466234\n",
      "Epoch: 97 reconstruction error: 0.463871\n",
      "Epoch: 98 reconstruction error: 0.463943\n",
      "Epoch: 99 reconstruction error: 0.464196\n",
      "Epoch: 100 reconstruction error: 0.466396\n",
      "Epoch: 101 reconstruction error: 0.465512\n",
      "Epoch: 102 reconstruction error: 0.463672\n",
      "Epoch: 103 reconstruction error: 0.464141\n",
      "Epoch: 104 reconstruction error: 0.462266\n",
      "Epoch: 105 reconstruction error: 0.464015\n",
      "Epoch: 106 reconstruction error: 0.462428\n",
      "Epoch: 107 reconstruction error: 0.465927\n",
      "Epoch: 108 reconstruction error: 0.461093\n",
      "Epoch: 109 reconstruction error: 0.464719\n",
      "Epoch: 110 reconstruction error: 0.465909\n",
      "Epoch: 111 reconstruction error: 0.458911\n",
      "Epoch: 112 reconstruction error: 0.462247\n",
      "Epoch: 113 reconstruction error: 0.461021\n",
      "Epoch: 114 reconstruction error: 0.467063\n",
      "Epoch: 115 reconstruction error: 0.466053\n",
      "Epoch: 116 reconstruction error: 0.464340\n",
      "Epoch: 117 reconstruction error: 0.462897\n",
      "Epoch: 118 reconstruction error: 0.465855\n",
      "Epoch: 119 reconstruction error: 0.462680\n",
      "Epoch: 120 reconstruction error: 0.464935\n",
      "Epoch: 121 reconstruction error: 0.460372\n",
      "Epoch: 122 reconstruction error: 0.463276\n",
      "Epoch: 123 reconstruction error: 0.461923\n",
      "Epoch: 124 reconstruction error: 0.466414\n",
      "Epoch: 125 reconstruction error: 0.463654\n",
      "Epoch: 126 reconstruction error: 0.464881\n",
      "Epoch: 127 reconstruction error: 0.464701\n",
      "Epoch: 128 reconstruction error: 0.464556\n",
      "Epoch: 129 reconstruction error: 0.465476\n",
      "Epoch: 130 reconstruction error: 0.464304\n",
      "Epoch: 131 reconstruction error: 0.467532\n",
      "Epoch: 132 reconstruction error: 0.462969\n",
      "Epoch: 133 reconstruction error: 0.463221\n",
      "Epoch: 134 reconstruction error: 0.464177\n",
      "Epoch: 135 reconstruction error: 0.460949\n",
      "Epoch: 136 reconstruction error: 0.463294\n",
      "Epoch: 137 reconstruction error: 0.463258\n",
      "Epoch: 138 reconstruction error: 0.465801\n",
      "Epoch: 139 reconstruction error: 0.466847\n",
      "Epoch: 140 reconstruction error: 0.466504\n",
      "Epoch: 141 reconstruction error: 0.467857\n",
      "Epoch: 142 reconstruction error: 0.464448\n",
      "Epoch: 143 reconstruction error: 0.460480\n",
      "Epoch: 144 reconstruction error: 0.465079\n",
      "Epoch: 145 reconstruction error: 0.464719\n",
      "Epoch: 146 reconstruction error: 0.465855\n",
      "Epoch: 147 reconstruction error: 0.462374\n",
      "Epoch: 148 reconstruction error: 0.462302\n",
      "Epoch: 149 reconstruction error: 0.462338\n",
      "Epoch: 150 reconstruction error: 0.461057\n",
      "Epoch: 151 reconstruction error: 0.464051\n",
      "Epoch: 152 reconstruction error: 0.468813\n",
      "Epoch: 153 reconstruction error: 0.465675\n",
      "Epoch: 154 reconstruction error: 0.469174\n",
      "Epoch: 155 reconstruction error: 0.463077\n",
      "Epoch: 156 reconstruction error: 0.461760\n",
      "Epoch: 157 reconstruction error: 0.463366\n",
      "Epoch: 158 reconstruction error: 0.458225\n",
      "Epoch: 159 reconstruction error: 0.463979\n",
      "Epoch: 160 reconstruction error: 0.465548\n",
      "Epoch: 161 reconstruction error: 0.463961\n",
      "Epoch: 162 reconstruction error: 0.460498\n",
      "Epoch: 163 reconstruction error: 0.462211\n",
      "Epoch: 164 reconstruction error: 0.462716\n",
      "Epoch: 165 reconstruction error: 0.463690\n",
      "Epoch: 166 reconstruction error: 0.464755\n",
      "Epoch: 167 reconstruction error: 0.464105\n",
      "Epoch: 168 reconstruction error: 0.467352\n",
      "Epoch: 169 reconstruction error: 0.466631\n",
      "Epoch: 170 reconstruction error: 0.465657\n",
      "Epoch: 171 reconstruction error: 0.466613\n",
      "Epoch: 172 reconstruction error: 0.463961\n",
      "Epoch: 173 reconstruction error: 0.463600\n",
      "Epoch: 174 reconstruction error: 0.465314\n",
      "Epoch: 175 reconstruction error: 0.466071\n",
      "Epoch: 176 reconstruction error: 0.467767\n",
      "Epoch: 177 reconstruction error: 0.465440\n",
      "Epoch: 178 reconstruction error: 0.464250\n",
      "Epoch: 179 reconstruction error: 0.466071\n",
      "Epoch: 180 reconstruction error: 0.465765\n",
      "Epoch: 181 reconstruction error: 0.464141\n",
      "Epoch: 182 reconstruction error: 0.465476\n",
      "Epoch: 183 reconstruction error: 0.464520\n",
      "Epoch: 184 reconstruction error: 0.465765\n",
      "Epoch: 185 reconstruction error: 0.462139\n",
      "Epoch: 186 reconstruction error: 0.464484\n",
      "Epoch: 187 reconstruction error: 0.465296\n",
      "Epoch: 188 reconstruction error: 0.462933\n",
      "Epoch: 189 reconstruction error: 0.463817\n",
      "Epoch: 190 reconstruction error: 0.462843\n",
      "Epoch: 191 reconstruction error: 0.464538\n",
      "Epoch: 192 reconstruction error: 0.466540\n",
      "Epoch: 193 reconstruction error: 0.463672\n",
      "Epoch: 194 reconstruction error: 0.463348\n",
      "Epoch: 195 reconstruction error: 0.463727\n",
      "Epoch: 196 reconstruction error: 0.464683\n",
      "Epoch: 197 reconstruction error: 0.461959\n",
      "Epoch: 198 reconstruction error: 0.463023\n",
      "Epoch: 199 reconstruction error: 0.466360\n"
     ]
    }
   ],
   "source": [
    "outputList = []\n",
    "error_list = []\n",
    "\n",
    "#For each RBM in out list\n",
    "for i in range(0, len(rbm_list)):\n",
    "    print('RBM', i+1)\n",
    "    #Train new RBM\n",
    "    rbm = rbm_list[i]\n",
    "    err = rbm.train(inputX)\n",
    "    error_list.append(err)\n",
    "\n",
    "    #Return output layer\n",
    "    #sess.run(out), sess.run(visibleGen), sess.run(hiddenGen)\n",
    "    outputX, reconstructedX, hiddenX = rbm.rbm_output(inputX)\n",
    "    outputList.append(outputX)\n",
    "    inputX= hiddenX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1\n",
    "for err in error_list:\n",
    "    print(\"RBM\",i)\n",
    "    pd.Series(err).plot(logy=False)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Reconstruction Error\")\n",
    "    plt.show()\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputX = np.array(train_X)\n",
    "inputX = inputX.astype(np.float32)\n",
    "rbmOne = rbm_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('RBM 1')\n",
    "outputX_rbmOne, reconstructedX_rbmOne, hiddenX_rbmOne = rbmOne.rbm_output(inputX)\n",
    "reconstructedX_rbmOne = pd.DataFrame(data=reconstructedX_rbmOne, index=train_X.index)\n",
    "for j in range(0,1):\n",
    "    example = j\n",
    "    print(\"Data generated by First RBM Layer\")\n",
    "    view_values(reconstructedX_rbmOne, train_Y, example)\n",
    "    print(\"Original Data\")\n",
    "    view_values(train_X, train_Y, example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructedX_rbmOne.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DBN(object):\n",
    "    def __init__(self, original_input_size, input_size, output_size, \n",
    "                 learning_rate, epochs, batchsize, rbmOne, rbmTwo, rbmThree):\n",
    "        # Define hyperparameters\n",
    "        self._original_input_size = original_input_size\n",
    "        self._input_size = input_size\n",
    "        self._output_size = output_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.batchsize = batchsize\n",
    "        self.rbmOne = rbmOne\n",
    "        self.rbmTwo = rbmTwo\n",
    "        self.rbmThree = rbmThree\n",
    "    \n",
    "        self.w = np.zeros([input_size, output_size], \"float\")\n",
    "        self.hb = np.zeros([output_size], \"float\")\n",
    "        self.vb = np.zeros([input_size], \"float\")\n",
    "    \n",
    "    def prob_h_given_v(self, visible, w, hb):\n",
    "        return tf.nn.sigmoid(tf.matmul(visible, w) + hb)\n",
    "\n",
    "    def prob_v_given_h(self, hidden, w, vb):\n",
    "        return tf.nn.sigmoid(tf.matmul(hidden, tf.transpose(w)) + vb)\n",
    "    \n",
    "    def sample_prob(self, probs):\n",
    "        return tf.nn.relu(tf.sign(probs - tf.random_uniform(tf.shape(probs))))\n",
    "\n",
    "    def train(self, X):\n",
    "        _w = tf.placeholder(\"float\", [self._input_size, self._output_size])\n",
    "        _hb = tf.placeholder(\"float\", [self._output_size])\n",
    "        _vb = tf.placeholder(\"float\", [self._input_size])\n",
    "        \n",
    "        prv_w = np.zeros([self._input_size, self._output_size], \"float\")\n",
    "        prv_hb = np.zeros([self._output_size], \"float\")\n",
    "        prv_vb = np.zeros([self._input_size], \"float\")\n",
    "        \n",
    "        cur_w = np.zeros([self._input_size, self._output_size], \"float\")\n",
    "        cur_hb = np.zeros([self._output_size], \"float\")\n",
    "        cur_vb = np.zeros([self._input_size], \"float\")\n",
    "        \n",
    "        v0 = tf.placeholder(\"float\", [None, self._original_input_size])\n",
    "\n",
    "        forwardOne = tf.nn.relu(tf.sign(tf.nn.sigmoid(tf.matmul(v0, self.rbmOne.w) + self.rbmOne.hb) - tf.random_uniform(tf.shape(tf.nn.sigmoid(tf.matmul(v0, self.rbmOne.w) + self.rbmOne.hb)))))\n",
    "        forwardTwo = tf.nn.relu(tf.sign(tf.nn.sigmoid(tf.matmul(forwardOne, self.rbmTwo.w) + self.rbmTwo.hb) - tf.random_uniform(tf.shape(tf.nn.sigmoid(tf.matmul(forwardOne, self.rbmTwo.w) + self.rbmTwo.hb)))))\n",
    "        forward = tf.nn.relu(tf.sign(tf.nn.sigmoid(tf.matmul(forwardTwo, self.rbmThree.w) + self.rbmThree.hb) - tf.random_uniform(tf.shape(tf.nn.sigmoid(tf.matmul( forwardTwo, self.rbmThree.w) + self.rbmThree.hb)))))\n",
    "        h0 = self.sample_prob(self.prob_h_given_v(forward, _w, _hb))\n",
    "        v1 = self.sample_prob(self.prob_v_given_h(h0, _w, _vb))\n",
    "        h1 = self.prob_h_given_v(v1, _w, _hb)\n",
    "        \n",
    "        positive_grad = tf.matmul(tf.transpose(forward), h0)\n",
    "        negative_grad = tf.matmul(tf.transpose(v1), h1)\n",
    "        \n",
    "        update_w = _w + self.learning_rate * (positive_grad - negative_grad) / tf.to_float(tf.shape(forward)[0])\n",
    "        update_vb = _vb +  self.learning_rate * tf.reduce_mean(forward - v1, 0)\n",
    "        update_hb = _hb +  self.learning_rate * tf.reduce_mean(h0 - h1, 0)\n",
    "        \n",
    "        backwardOne = tf.nn.relu(tf.sign(tf.nn.sigmoid(tf.matmul(v1, self.rbmThree.w.T) + self.rbmThree.vb) - tf.random_uniform(tf.shape(tf.nn.sigmoid(tf.matmul(v1, self.rbmThree.w.T) + self.rbmThree.vb)))))\n",
    "        backwardTwo = tf.nn.relu(tf.sign(tf.nn.sigmoid(tf.matmul(backwardOne, self.rbmTwo.w.T) + self.rbmTwo.vb) - tf.random_uniform(tf.shape(tf.nn.sigmoid(tf.matmul(backwardOne, self.rbmTwo.w.T) + self.rbmTwo.vb)))))\n",
    "        backward = tf.nn.relu(tf.sign(tf.nn.sigmoid(tf.matmul(backwardTwo, self.rbmOne.w.T) + self.rbmOne.vb) - tf.random_uniform(tf.shape(tf.nn.sigmoid(tf.matmul(backwardTwo, self.rbmOne.w.T) + self.rbmOne.vb)))))\n",
    "        \n",
    "        err = tf.reduce_mean(tf.square(v0 - backward))\n",
    "        error_list = []\n",
    "        \n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "\n",
    "            for epoch in range(self.epochs):\n",
    "                for start, end in zip(range(0, len(X), self.batchsize), range(self.batchsize,len(X), self.batchsize)):\n",
    "                    batch = X[start:end]\n",
    "                    cur_w = sess.run(update_w, feed_dict={v0: batch, _w: prv_w, _hb: prv_hb, _vb: prv_vb})\n",
    "                    cur_hb = sess.run(update_hb, feed_dict={v0: batch, _w:  prv_w, _hb: prv_hb, _vb: prv_vb})\n",
    "                    cur_vb = sess.run(update_vb, feed_dict={v0: batch, _w: prv_w, _hb: prv_hb, _vb: prv_vb})\n",
    "                    prv_w = cur_w\n",
    "                    prv_hb = cur_hb\n",
    "                    prv_vb = cur_vb\n",
    "                error = sess.run(err, feed_dict={v0: X, _w: cur_w, _vb: cur_vb, _hb: cur_hb})\n",
    "                print ('Epoch: %d' % (epoch+1),'reconstruction error: %f' % error)\n",
    "                error_list.append(error)\n",
    "            self.w = prv_w\n",
    "            self.hb = prv_hb\n",
    "            self.vb = prv_vb\n",
    "            return error_list\n",
    "        \n",
    "    def dbn_output(self, X):\n",
    "\n",
    "        input_X = tf.constant(X)\n",
    "        forwardOne = tf.nn.sigmoid(tf.matmul(input_X, self.rbmOne.w) + self.rbmOne.hb)\n",
    "        forwardTwo = tf.nn.sigmoid(tf.matmul(forwardOne, self.rbmTwo.w) + self.rbmTwo.hb)\n",
    "        forward = tf.nn.sigmoid(tf.matmul(forwardTwo, self.rbmThree.w) + self.rbmThree.hb)\n",
    "\n",
    "        _w = tf.constant(self.w)\n",
    "        _hb = tf.constant(self.hb)\n",
    "        _vb = tf.constant(self.vb)\n",
    "\n",
    "        out = tf.nn.sigmoid(tf.matmul(forward, _w) + _hb)\n",
    "        hiddenGen = self.sample_prob(self.prob_h_given_v(forward, _w, _hb))\n",
    "        visibleGen = self.sample_prob(self.prob_v_given_h(hiddenGen, _w, _vb))\n",
    "\n",
    "        backwardTwo = tf.nn.sigmoid(tf.matmul(visibleGen, self.rbmThree.w.T) + self.rbmThree.vb)\n",
    "        backwardOne = tf.nn.sigmoid(tf.matmul(backwardTwo, self.rbmTwo.w.T) + self.rbmTwo.vb)\n",
    "        backward = tf.nn.sigmoid(tf.matmul(backwardOne, self.rbmOne.w.T) + self.rbmOne.vb)\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            return sess.run(out), sess.run(backward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbn = DBN(14, 12, 12, 0.02, 50, 100, rbm_list[0], rbm_list[1], rbm_list[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputX = np.array(inputX)\n",
    "error_list = []\n",
    "error_list = dbn.train(inputX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"DBN\")\n",
    "pd.Series(error_list).plot(logy=False)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Reconstruction Error\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X.shape\n",
    "train_Y.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputX_dbn, reconstructedX_dbn = dbn.dbn_output(inputX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1.0/(1.0 + np.exp(-x))\n",
    "\n",
    "def hypothesis(X, theta) :\n",
    "    \"\"\" \n",
    "    X-entire array(m,n+1)\n",
    "    n+1^ 1 dummy feature Xo\n",
    "\n",
    "    theta- np.array(n+1,1)\n",
    "    \"\"\"\n",
    "    return sigmoid(np.dot(X, theta))\n",
    "\n",
    "def error(X,y,theta):\n",
    "    \"\"\"\n",
    "    params:\n",
    "    X:(m,n+1)\n",
    "    y:(m,1)\n",
    "    theta:(n+1,1)\n",
    "\n",
    "    return:scale_value=loss\n",
    "    \"\"\"\n",
    "    hi = hypothesis(X,theta)\n",
    "    error= -1* np.mean ( y * np.log(hi) + ( ( 1 - y ) * (np.log( 1 - hi )) ) )\n",
    "\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(X,y,theta):\n",
    "    \"\"\"\n",
    "    params:\n",
    "    X:(m,n+1)\n",
    "    y:(m,1)\n",
    "\n",
    "    theta:(n+1,1)\n",
    "     \n",
    "    return:gradient_vector: (n+1,1)\n",
    "    \"\"\"\n",
    "    hi = hypothesis(X,theta)\n",
    "\n",
    "    grad = np.dot(X.T,(y-hi))\n",
    "    m=X.shape[0]\n",
    "\n",
    "    return grad/m\n",
    "\n",
    "def gradient_descent(X,y,lr=0.02,max_itr=500):\n",
    "    \n",
    "    n=X.shape[1] \n",
    "    theta = np.zeros((n,1))\n",
    "    \n",
    "    error_list= []\n",
    "    \n",
    "    for i in range(max_itr):\n",
    "        err = error(X,y,theta)\n",
    "        error_list.append(err)\n",
    "\n",
    "        grad = gradient(X,y,theta)\n",
    "        #update theta\n",
    "        theta = theta + lr * grad\n",
    "    return (theta, error_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ones = np.ones((outputX_dbn.shape[0],1))\n",
    "X_New_Train = np.hstack((ones,outputX_dbn))\n",
    "X_New_Train = X_New_Train[:4000,:]\n",
    "Y_Train= Y_Train.reshape((-1,1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
